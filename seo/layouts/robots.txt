{{- /*
	MODULE: SEO
	Robots.txt
	---
	What it says on the tin. Note site.Params.Seo.IsPrivate is handled by robots
	tags on each page rather than here. Non-production environments disallow all
	as a blanket "safety" measure, but you should probably block access to your
	live staging website with a password or firewall.

	@link https://runtimeterror.dev/dynamic-robots-txt-hugo-external-data-sources/
*/ -}}

# ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡇⠀⢸⣗⠲⣖⢤⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
# ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠇⠀⠀⢼⡏⠀⠀⠉⠲⡽⡀⠀⠀⠀⢀⢀⣠⣤⣄⡀⡀⢰⣶⣶⣶⣶⣶⣾⣿⡇
# ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⡏⠀⠀⠀⡇⣇⠀⠀⠀⠀⢸⣇⠀⠀⡐⡟⡻⠒⠙⢯⡿⡊⠀⠛⠛⡛⣿⡟⠋⠉⠁
# ⠀⠀⠀⠀⠀⠀⠀⠀⠀⣴⡏⠀⠀⠀⠀⡇⣿⠀⠀⢀⡠⢖⡇⠀⡧⣿⠁⠀⠀⠀⠀⡇⣿⠀⠀⠀⠀⣿⣇⡅⠀⠀
# ⠀⠀⠀⠀⠀⠀⠀⠀⣼⠃⠀⠀⠀⠀⠀⣿⡟⠻⠷⢯⣍⡀⠀⣼⣿⡇⠀⠀⠀⠀⠀⡇⡇⠀⠀⠀⠀⣿⣇⠁⠀⠀
# ⡀⡀⠀⠀⠀⠀⢠⣟⠇⠀⠀⠀⠀⠀⡸⡿⡇⠀⠀⠀⠀⠉⢧⣳⣻⠀⠀⠀⠀⠀⣼⣞⠇⠀⠀⠀⢈⠿⣿⠀⠀⠀
# ⢷⣇⠀⠀⠀⠠⣯⠃⠀⠀⠀⠀⠀⠀⢯⡯⡇⠀⠀⠀⠀⠀⣾⢹⣿⡀⠀⣀⣠⣴⡿⠁⠀⠀⠀⠀⢐⢏⡇⠀⠀⠀
# ⠈⣿⣆⠀⣴⡗⠁⠀⠀⠀⠀⠀⠀⢀⡇⣯⠀⠀⠀⣠⣶⣿⠏⠀⠛⣯⡯⡿⠋⠁⠀⠀⠀⠀⠀⠀⣯⣿⠇⠀⠀⠀
# ⠀⠘⣿⣟⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⣇⣸⡿⠛⠛⠛⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠘⠀⠀⠀
# ⠀⠀⠘⠻⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀

{{- /* Comment everything out by changing this to "#" */ -}}
{{ $n := "" -}}

{{ if hugo.IsDevelopment }}
{{$n}}User-agent: *
{{$n}}Disallow: /
{{- end -}}

{{- if site.Params.Seo.Ai.Block -}}
	{{- $url := "https://raw.githubusercontent.com/ai-robots-txt/ai.robots.txt/main/robots.json" -}}
	{{- with try (resources.GetRemote $url) -}}
		{{- with .Err -}}
			{{- warnf "SEO: %s" . -}}
		{{- else with .Value -}}
			{{- $robots := unmarshal .Content -}}
			{{- range $botname, $_ := $robots }}
				{{- if not (in site.Params.Seo.Ai.Whitelist $botname) }}
					{{- printf "%sUser-agent: %s\n" $n $botname }}
				{{- end }}
			{{- end -}}
			{{- printf "Disallow: /\n" }}
			{{- warnf "SEO: Bad bots bundled by https://github.com/ai-robots-txt/ai.robots.txt" }}
		{{- end -}}
	{{- else -}}
		{{- warnf "SEO: Unable to get remote resource %q" $url -}}
	{{- end -}}
{{- end }}

{{$n}}User-agent: bingbot
{{$n}}User-agent: SemrushBot
{{$n}}Crawl-delay: 30

{{ if not site.Params.Seo.IsPrivate -}}
{{- with .Sitemap.Filename }}
{{$n}}Sitemap: {{ path.Join site.BaseURL . }}
{{- end }}
{{- end }}
